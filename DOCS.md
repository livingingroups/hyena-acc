# Documentation

This document provides details about how to analyse the spotted hyena telemetry
data using the pipeline I built. If you have not yet read README.md, I
recommend reading that before proceeding with this document.

If you are a new user who just wants to try using this code, and are not much
interested in modifying it, please skip straight to the end.

# Notes
	- This software is built in and for linux command line execution. This
	  documentation will assume you are using a linux terminal running bash
	  for your data analysis. If you are using some other method of
	  executing this software, make sure you have read the code and made the
	  appropriate changes.
	- The documentation is written for someone who is familiar with the
	  Python programming language, but the code is written in a way that
	  anyone can easily run it. A TL;DR version of the documentation that
	  explains running the code for a five-year-old is provided at the ending.
	- This software is completely open-source, as are all components used by
	  it. Therefore, all bug fixing is possible in theory. But as a great
	  jedi once said, 'there's always a bigger fish.'


# Setting up
Set up the following directory structure where you want to install your
software. Ensure that there is sufficient space in the partition, because some
of the data files generated (e.g. by Feature Extraction) are enormous and can
take up up to 10 GiB of space.

```
	(Any Empty Directory)
	├── code
	├── Data
	│   ├── ClassificationsInTotal
	│   ├── ClassifierPerformanceResults
	│   ├── ExtractedFeatures
	│   └── FeaturesInTotal
	└── Figures
```

In the code/ directory, you need at least the following files in order to run
the pipeline:
	- config.py
	- variables.py
	- errors.py
	- handling_hyena_hdf5.py
	- read_audits.py
	- crawler.py
	- extractor.py
	- features.py
	- classifier.py
	- analyses.py

Make sure you have all these files in the code/ directory before proceeding.

Also ensure that you have the behavioural audits (in my case those generated by
Max) stored in a known location.

The most important ingredient is, of course, the data itself. Create a folder
with hdf5 files for all hyenas stored neatly, preferably at 25 Hz.


# Execution
Edit and configure the config.py and variables.py files to set up exactly what
the code does. You can choose what behaviours are considered 'States' as opposed
to 'Events', and also minor (but important!) details like where the data is
stored. The variable WINDOW_DURATION in config.py is very important, as it
decides the duration of the shortest interval for which behavioural
classification is done.

A crawler object, defined in crawler.py, is an easy way to load and access
timestamped data from the hdf5 files. Each crawler object will contain the
following four attributes
	- crawler.surge
	- crawler.heave
	- crawler.sway
	- crawler.vedba 
which are lists containing data points of the triaxial accelerometer and its
vectorial dynamic body acceleration that span the duration specified by
WINDOW_DURATION. For example, if my WINDOW_DURATION is 3 seconds, and my
accelerometer frequency is 25 Hz, each of the above lists will be of length 25*3
= 75. 

The file features.py contains functions that take the crawler object as
argument, which are then used in feature extraction by extractor.py . 16 default
features are provided in this file. All functions beginning with an underscore are
considered to be internal and ignored.

classifier.py defines the class `ClasifierBundle`, which allows for the
simultaneous training and prediction of several classifiers in one command. It
also decides the best classifier automatically. 

The files extractor.py and analyses.py provide usable functionalities. Each
functionality is a python function, and a number of them can be found commented
at the end of each. To run any particular functionality, uncomment that
particular line. Descriptions and meanings of each function is provided as a
docstring where the function is defined.

# Result interpretation
All results are stored as text files in the Data/ subdirectories. All figures are
stored in the png and pdf/pgf formats in the Figures/ directory.
Almost every text file contains LaTeX copyable code that can be incorporated
directly in reports or manuscripts. 

All extracted features and classifications are stored as .csv files that can be
used for your own biological analyses.

For classifier performance evaluation, the code automatically provides
precision, recall, overall accuracy, and confusion matrices for all chosen
classifiers. This is done for randomised, auditwise, and individualwise testing.


# Frequently faced errors
This section describes errors that can't easily be understood and fixed by
looking at the Python traceback report.

`MemoryError`: This is the most common error that occurs especially during feature
extraction. What this means is that you've run out of RAM. A good idea is to
restart your computer and run only the extractor.py code in that session. This
worked for me with 8GiB of RAM.

Random NaNs in VeDBA variance: This is a random bug, probably based on the way
the VeDBA variance feature function is defined in features.py. I found only 8
occurrences of this in all of the data. I dealt with it by removing the
offending lines directly using sed. You can do it using a text editor, but be
warned, the files are enormous!

# TL;DR
To run this code, 
	- put all the code in the code directory,
	- edit config.py and variables.py,
	- edit features.py and add necessary features,
	- run extractor.py after uncommenting the extraction functionalities at
	  the end of the file.
	- run analyses.py after uncommenting the analysis functionalities at the
	  end of the file.

That's it!

--------------------------------------------------------------------------------

# New users

If you are an inexperienced user trying to get this system to run, please follow
these steps.

1. Download and clone this git repository, and set up with following structure
   of folders, with this code in the `code` directory:

    ```
      (Any Empty Directory)
      ├── code
      ├── Data
      │   ├── ClassificationsInTotal
      │   ├── ClassifierPerformanceResults
      │   ├── ExtractedFeatures
      │   └── FeaturesInTotal
      └── Figures
    ```

2. Two other variables are needed. One contains the hdf5 files, and one contains
   audit data. Because of historical reasons with the way data came into
   this project from various sources, several variables need to be
   configured for this. First open `variables.py`, and then do this as follows:

   - Set up HDD_MNT_PNT and D_hdf5, such that in combination (HDD_MNT_POINT + D_hdf5) they point to
     the directory containing the hdf5 files. Make sure both variables end in `/` (or \ if
     you're using Windows.)

   - Likewise, ensure that PROJECTROOT is set to the directory you
     initialised above (the one containing code, Data, and all other
     directories).

   - Also due to historical reasons, audits need to be placed in the
     directory that is given by the combination of DROPBOXROOT and AUDITS,
     making sure that they both end in / or \ respectively. Note that there
     is no need for Dropbox to actually be involved. This naming exists
     because this data was routinely updated on DROPBOX as this code took
     form.

   Apologies for this horrendous step. This is circumvented entirely in future
   projects using the `pathlib` or `os.join` modules, wherein only one file,
   PROJECTROOT, will need to be defined.

3. Now we will get around to actually running the code. 

   - First, you need to open
     the file `extractor.py`, scroll to the bottom, and uncomment the line needed
     for what you need to do. You can uncomment both of these, but note that a lot
     of RAM may be used. Run this file with `python3 extractor.py`

   - Second, open `analyses.py`. You need to scroll to the bottom, and choose
     what you need to do by uncommenting specific lines. At the very least,
     uncomment the lines running the functions `generate_combined_data_files()`,
     `get_metrics_for_randomised_testing()`, and the other two
     `get_metrics_for...` functions. Then run this file with `python3
     analyses.py`

   - Third, and finally, you need to open `biology.py`. This code has plenty of
     functions at the bottom, and each of them generates a figure. This file
     contains all biological analyses we performed, some of which are used in
     the paper. A piece of advice: when you run this code with `python3
     biology.py`, make sure you uncomment only one function (bottom of the file)at a time. For generating the figures in the paper, different matplotlib settings have been used for different figures, some of which may conflict with each other. 


