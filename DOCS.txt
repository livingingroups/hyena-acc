This document provides details about how to analyse the spotted hyena telemetry
data using the pipeline I built. If you have not yet read README.txt, I
recommend reading that before proceeding with this document.


NOTES
	- This software is built in and for linux command line execution. This
	  documentation will assume you are using a linux terminal running bash
	  for your data analysis. If you are using some other method of
	  executing this software, make sure you have read the code and made the
	  appropriate changes.
	- The documentation is written for someone who is familiar with the
	  Python programming language, but the code is written in a way that
	  anyone can easily run it. A TL;DR version of the documentation that
	  explains running the code for a five-year-old is provided at the ending.
	- This software is completely open-source, as are all components used by
	  it. Therefore, all bug fixing is possible in theory. But as a great
	  jedi once said, `there's always a bigger fish.'


SETTING UP
Set up the following directory structure where you want to install your
software. Ensure that there is sufficient space in the partition, because some
of the data files generated (e.g. by Feature Extraction) are enormous and can
take up up to 10 GiB of space.

	(Any Empty Directory)
	├── code
	├── Data
	│   ├── ClassificationsInTotal
	│   ├── ClassifierPerformanceResults
	│   ├── ExtractedFeatures
	│   └── FeaturesInTotal
	└── Figures

In the code/ directory, you need at least the following files in order to run
the pipeline:
	- config.py
	- variables.py
	- errors.py
	- handling_hyena_hdf5.py
	- read_audits.py
	- crawler.py
	- extractor.py
	- features.py
	- classifier.py
	- analyses.py
Make sure you have all these files in the code/ directory before proceeding.

Also ensure that you have the behavioural audits (in my case those generated by
Max) stored in a known location.

The most important ingredient is, of course, the data itself. Create a folder
with hdf5 files for all hyenas stored neatly, preferably at 25 Hz.


EXECUTION
Edit and configure the config.py and variables.py files to set up exactly what
the code does. You can choose what behaviours are considered 'States' as opposed
to 'Events', and also minor (but important!) details like where the data is
stored. The variable WINDOW_DURATION in config.py is very important, as it
decides the duration of the shortest interval for which behavioural
classification is done.

A crawler object, defined in crawler.py, is an easy way to load and access
timestamped data from the hdf5 files. Each crawler object will contain the
following four attributes
	- crawler.surge
	- crawler.heave
	- crawler.sway
	- crawler.vedba 
which are lists containing data points of the triaxial accelerometer and its
vectorial dynamic body acceleration that span the duration specified by
WINDOW_DURATION. For example, if my WINDOW_DURATION is 3 seconds, and my
accelerometer frequency is 25 Hz, each of the above lists will be of length 25*3
= 75. 

The file features.py contains functions that take the crawler object as
argument, which are then used in feature extraction by extractor.py . 16 default
features are provided in this file. All functions beginning with '_' are
considered to be internal and ignored.

classifier.py defines the class ClasifierBundle, which allows for the
simultaneous training and prediction of several classifiers in one command. It
also decides the best classifier automatically. 

The files extractor.py and analyses.py provide usable functionalities. Each
functionality is a python function, and a number of them can be found commented
at the end of each. To run any particular functionality, uncomment that
particular line. Descriptions and meanings of each function is provided as a
docstring where the function is defined.

RESULT INTERPRETATION
All results are stored as text files in the Data/ subdirectories. All figures are
stored in the png and pgf (LaTeX readable) formats in the Figures/ directory.
Almost every text file contains LaTeX copiable code that can be incorporated
directly in reports or manuscripts. 

All extracted features and classifications are stored as csv files that can be
used for your own biological analyses.

For classifier performance evaluation, the code automatically provides
precision, recall, overall accuracy, and confusion matrices for all chosen
classifiers. This is done for randomised, auditwise, and individualwise testing.


FREQUENTLY FACED ERRORS
This section describes errors that can't easily be understood and fixed by
looking at the Python traceback report.

MemoryError: This is the most common error that occurs especially during feature
extraction. What this means is that you've run out of RAM. A good idea is to
restart your computer and run only the extractor.py code in that session. This
worked for me with 8GiB of RAM.

Random NaNs in VeDBA variance: This is a random bug, probably based on the way
the VeDBA variance feature function is defined in features.py. I found only 8
occurrences of this in all of the data. I dealt with it by removing the
offending lines directly using sed. You can do it using a text editor, but be
warned, the files are enormous!

TL;DR
To run this code, 
	- put all the code in the code directory,
	- edit config.py and variables.py,
	- edit features.py and add necessary features,
	- run extractor.py after uncommenting the extraction functionalities at
	  the end of the file.
	- run analyses.py after uncommenting the analysis functionalities at the
	  end of the file.

That's it!

--------------------------------------------------------------------------------
